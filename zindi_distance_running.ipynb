{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "zindi distance running.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNG73Ult_jar"
      },
      "source": [
        "this was for hackathon hosted on zindi for Egyptian universities \r\n",
        "this is its link \r\n",
        "https://zindi.africa/hackathons/umojahack-egypt/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWnLG2JTvx3h"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('./gdrive')\n",
        "\n",
        "# !cp \"./gdrive/MyDrive/UmojaHackEgypt.zip\" .\n",
        "# !cp \"./gdrive/MyDrive/train_tracks.zip\" .\n",
        "\n",
        "# !unzip \"/content/train_targets.zip\"\n",
        "# !7z e -pqkptu \"/content/UmojaHackEgypt.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wbdfxm1_YM1"
      },
      "source": [
        "# !pip install catboost\r\n",
        "# !pip install pytorch-tabnet\r\n",
        "# !unzip  \"/content/train_sequences.zip\"\r\n",
        "# !unzip \"/content/train_tracks.zip\"\r\n",
        "# !unzip \"/content/test_sequences.zip\"\r\n",
        "# !pip install traces\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZZr4w32y-Nb"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np \r\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\r\n",
        "import glob, random\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from xgboost import XGBRegressor\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.multioutput import MultiOutputRegressor\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.tree import DecisionTreeRegressor\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "from catboost import CatBoostRegressor\r\n",
        "from catboost import Pool\r\n",
        "from sklearn.neighbors import KNeighborsRegressor\r\n",
        "import traces\r\n",
        "\r\n",
        "\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaMYu-IPxeed"
      },
      "source": [
        "sequences = sorted(glob.glob('train_sequences/*source.csv')) \r\n",
        "targets = [s.replace('sequence', 'target').replace('source', 'target') for s in sequences]\r\n",
        "tracks = sorted(glob.glob('train_tracks/*source.csv'))\r\n",
        "tracksbench = [t.replace(\"source\",\"bench\") for  t in tracks]\r\n",
        "# sequences = sequences +tracks\r\n",
        "# targets = targets + tracksbench"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqzjIXXAtx0G",
        "outputId": "75fb8c3e-df58-4239-e155-bea6b57dde53"
      },
      "source": [
        "uni = []\r\n",
        "for i in range(len(sequences)):\r\n",
        "  uni.extend(pd.read_csv(sequences[1]).Movement_Type.unique())\r\n",
        "print(pd.Series(uni).unique ())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR8mheoZwncX",
        "outputId": "be01b1ba-eb01-4b2e-8cde-d507a71df8da"
      },
      "source": [
        "pd.Series([3,np.nan,np.nan,3,np.nan,4,7,8]).interpolate(method='linear')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    3.0\n",
              "1    3.0\n",
              "2    3.0\n",
              "3    3.0\n",
              "4    3.5\n",
              "5    4.0\n",
              "6    7.0\n",
              "7    8.0\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebqKhIWUVw7w"
      },
      "source": [
        "def firstindex(series):\r\n",
        "  return series[series.notnull()].index[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1il5wDB0Gmq"
      },
      "source": [
        "timesteps = 20\r\n",
        "def load_seq(i,timesteps= timesteps):\r\n",
        "    source = pd.read_csv(sequences[i])\r\n",
        "    target = pd.read_csv(targets[i])\r\n",
        "    original_sub = source.copy()\r\n",
        "    source.Time = np.round(source.Time).astype(\"int\")\r\n",
        "    source = source.drop_duplicates(subset='Latitude')\r\n",
        "    preds = pd.DataFrame({\r\n",
        "        'Time':range(-timesteps,min(len(original_sub),len(target)))\r\n",
        "    })\r\n",
        "    # Create traces timeseries for both Latitude and Longitude\r\n",
        "    lat_ts = traces.TimeSeries([s for s in source[['Time', 'Latitude']].values])\r\n",
        "    lon_ts = traces.TimeSeries([s for s in source[['Time', 'Longitude']].values])\r\n",
        "    \r\n",
        "    # Resample with linear interpolation and store results\r\n",
        "    preds['Latitude'] = [source.Latitude[firstindex(source.Latitude)]]*timesteps + [x[1] for x in lat_ts.sample(sampling_period=1,interpolate='linear', start=0, end=min(len(original_sub),len(target))-1 )]\r\n",
        "    preds['Longitude'] = [source.Longitude[firstindex(source.Longitude)]]*timesteps+ [x[1] for x in lon_ts.sample(sampling_period=1,interpolate='linear', start=0, end=min(len(original_sub),len(target))-1)]\r\n",
        "\r\n",
        "    preds['Accuracy'] = original_sub.Accuracy.tolist()[:1] * timesteps + original_sub.Accuracy.tolist()[:min(len(original_sub),len(target))]\r\n",
        "\r\n",
        "    preds['Movement_Type'] = original_sub.Movement_Type.tolist()[:1] * timesteps + original_sub.Movement_Type.tolist()[:min(len(original_sub),len(target))]\r\n",
        "    target.columns= [ \"Time\", \"trueLat\",\"trueLong\"]\r\n",
        "    newTarget = pd.DataFrame({\r\n",
        "        'Time':range(-timesteps,min(len(original_sub),len(target)))\r\n",
        "    })\r\n",
        "    target.drop(\"Time\",axis=1,inplace=True)\r\n",
        "    \r\n",
        "    newTarget[\"trueLat\"]  = target.trueLat.tolist()[:1] * timesteps + target.trueLat.tolist()[:min(len(original_sub),len(target))]\r\n",
        "    newTarget[\"trueLong\"]  = target.trueLong.tolist()[:1] * timesteps + target.trueLong.tolist()[:min(len(original_sub),len(target))]\r\n",
        "    newTarget.drop(\"Time\",axis=1,inplace=True)\r\n",
        "    preds.drop(\"Time\",axis=1,inplace=True)\r\n",
        "    npseq = pd.concat([preds, newTarget],axis=1).to_numpy()\r\n",
        "\r\n",
        "    seqdatax = []\r\n",
        "    seqdatay = []\r\n",
        "    for i in range(len(npseq) -timesteps ):\r\n",
        "      seqdatax.append(npseq[i:i+timesteps,:-2])\r\n",
        "      seqdatay.append(npseq[i+timesteps-1,-2:])\r\n",
        "    return np.array(seqdatax),np.array(seqdatay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6e7ic-Cw92o"
      },
      "source": [
        "datax, datay = load_seq(0,timesteps= timesteps)\r\n",
        "for i in range(1,len(targets)):\r\n",
        "  datax = np.concatenate((datax,load_seq(i)[0]) )\r\n",
        "  datay = np.concatenate((datay,load_seq(i)[1]))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4ideUJ0wDw_"
      },
      "source": [
        "class Flatten(nn.Module):\r\n",
        "    def forward(self, input):\r\n",
        "        batch_size = input.size(0)\r\n",
        "        out = input.view(batch_size,-1)\r\n",
        "        return out\r\n",
        "class MyTransformer(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(MyTransformer,self).__init__()\r\n",
        "    self.layers = nn.Sequential(\r\n",
        "        nn.TransformerEncoderLayer(5,1,dim_feedforward=128),\r\n",
        "        Flatten(),\r\n",
        "        nn.Dropout(0.2),\r\n",
        "        nn.PReLU(),\r\n",
        "        nn.Linear(25,25),\r\n",
        "        nn.PReLU(),\r\n",
        "        nn.Dropout(0.2),\r\n",
        "        nn.Linear(25,2)\r\n",
        "    )\r\n",
        "  def forward(self,x):\r\n",
        "    return self.layers(x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csBhMdw-KKva"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hncTDKWRIqy6"
      },
      "source": [
        "\"\"\"\r\n",
        "actually even if DL was promising at first it couldn't compete with other Tree based models (with stacking )\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "# model =  MyTransformer().to(device)\r\n",
        "\r\n",
        "# optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.01)\r\n",
        "\r\n",
        "# n_samples = len(dataxtrain)\r\n",
        "# batch_size= 128\r\n",
        "# better_batch_size = ceil(n_samples, ceil(n_samples, batch_size))\r\n",
        "# epochs= 50\r\n",
        "# for e in range(epochs):\r\n",
        "#   model.train()\r\n",
        "#   trainloss = []\r\n",
        "#   for i in range(ceil(n_samples, better_batch_size)): \r\n",
        "#     batchx = dataxtrain[i*better_batch_size :(i+1)*better_batch_size]\r\n",
        "#     batchy = dataytrain[i*better_batch_size :(i+1)*better_batch_size]\r\n",
        "#     model.zero_grad()\r\n",
        "#     batchy = batchy * 1000\r\n",
        "#     batchy = torch.Tensor(batchy).to(device)\r\n",
        "#     batchx = torch.Tensor(batchx).to(device)\r\n",
        "#     preds = model(batchx)\r\n",
        "#     loss = F.mse_loss(preds,batchy)\r\n",
        "#     loss.backward()\r\n",
        "#     trainloss.append(loss.item())\r\n",
        "#     optimizer.step()\r\n",
        "  \r\n",
        "#   model.eval()\r\n",
        "#   valLosses = []\r\n",
        "#   for i in range(ceil(len(dataxvalid), better_batch_size)): \r\n",
        "#     batchx = torch.Tensor(dataxvalid[i*better_batch_size:(i+1)*better_batch_size]).to(device)\r\n",
        "#     valpreds = model(batchx)\r\n",
        "#     valpreds = valpreds/ 1000\r\n",
        "#     lossStep = np.sqrt(mean_squared_error(datayvalid[i*better_batch_size:(i+1)*better_batch_size],valpreds.cpu().detach().numpy()))\r\n",
        "#     valLosses.append(lossStep)\r\n",
        "#   print(e,np.mean(valLosses),\"validation\",np.mean(trainloss),\"train\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQLTBH3UkW4k"
      },
      "source": [
        " 0.0009185077056372776 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bScnLzQrPxMI"
      },
      "source": [
        "ss = pd.read_csv(\"SampleSubmission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihG8smwLP60D"
      },
      "source": [
        "\r\n",
        "ss[\"cord\"] = ss[\"Row_ID\"].apply(lambda x : x.split(\" X \")[-1])\r\n",
        "ss[\"seq_id\"] = ss[\"Row_ID\"].apply(lambda x : x.split(\" X \")[0])\r\n",
        "ss[\"time\"] = ss[\"Row_ID\"].apply(lambda x : int(x.split(\" X \")[1][4:]))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCIBZsQjeYzc"
      },
      "source": [
        "ss = ss.sort_values([\"seq_id\",\"time\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7olw1-Jxudk",
        "outputId": "4baabefc-b9da-4cdb-9c1a-e9fd93ca45aa"
      },
      "source": [
        "datax[0,0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.00000000e+01, -1.46009872e-03,  4.29911647e-04,  1.64640007e+01,\n",
              "        2.00000000e+00])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQGP4tJtTPF5"
      },
      "source": [
        "def load_seq_test(seq_id,timesteps= timesteps):\r\n",
        "  \r\n",
        "    source = pd.read_csv(f'test_sequences/{seq_id}_source.csv')\r\n",
        "    source.Time = np.round(source.Time).astype(\"int\")\r\n",
        "    original_sub = source.copy()\r\n",
        "    \r\n",
        "    source = source.drop_duplicates(subset='Latitude')\r\n",
        "    preds = pd.DataFrame({\r\n",
        "        'Time':range(-timesteps,len(original_sub))\r\n",
        "    })\r\n",
        "    # Create traces timeseries for both Latitude and Longitude\r\n",
        "    lat_ts = traces.TimeSeries([s for s in source[['Time', 'Latitude']].values])\r\n",
        "    lon_ts = traces.TimeSeries([s for s in source[['Time', 'Longitude']].values])\r\n",
        "    \r\n",
        "    # Resample with linear interpolation and store results\r\n",
        "    preds['Latitude'] = [source.Latitude[firstindex(source.Latitude)]]*timesteps + [x[1] for x in lat_ts.sample(sampling_period=1,interpolate='linear', start=0, end=len(original_sub)-1 )]\r\n",
        "    preds['Longitude'] = [source.Longitude[firstindex(source.Longitude)]]*timesteps+ [x[1] for x in lon_ts.sample(sampling_period=1,interpolate='linear', start=0, end=len(original_sub)-1)]\r\n",
        "\r\n",
        "    preds['Accuracy'] = original_sub.Accuracy.tolist()[:1] * timesteps + original_sub.Accuracy.tolist()\r\n",
        "\r\n",
        "    preds['Movement_Type'] = original_sub.Movement_Type.tolist()[:1] * timesteps + original_sub.Movement_Type.tolist()\r\n",
        "    preds.drop(\"Time\",axis=1,inplace=True)\r\n",
        "    npseq = preds.to_numpy()\r\n",
        "    seqdatax = []\r\n",
        "  \r\n",
        "    for i in range(len(npseq) -timesteps ):\r\n",
        "      seqdatax.append(npseq[i:i+timesteps].tolist())\r\n",
        "    return np.array(seqdatax)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pT3ogiCXLtx"
      },
      "source": [
        "ss.to_csv(\"sub_weight_decay01_lr001.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DrOGhGJGviU"
      },
      "source": [
        "dataxtrain ,dataxvalid, dataytrain,datayvalid = train_test_split(datax,datay,test_size=0.3,shuffle=True)\r\n",
        "\r\n",
        "\r\n",
        "tempx = dataxtrain.copy()\r\n",
        "tempy = dataytrain.copy()\r\n",
        "\r\n",
        "dataxtrain = tempx[:int(0.5*len(tempx))]\r\n",
        "dataytrain = tempy[:int(0.5*len(tempx))]\r\n",
        "dataxtrain1 = tempx[int(0.5*len(tempx)):]\r\n",
        "dataytrain1 = tempy[int(0.5*len(tempx)):]\r\n",
        "\r\n",
        "\r\n",
        "halfxtrain = datax[:int(0.5*len(datax))]\r\n",
        "halfytrain = datay[:int(0.5*len(datax))]\r\n",
        "halfxvalid = datax[int(0.5*len(datax)):]\r\n",
        "halfyvalid = datay[int(0.5*len(datax)):]\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmGuhK5yGhoK",
        "outputId": "cc7fd972-8ac3-413f-f759-c915d9c8776c"
      },
      "source": [
        "np.sqrt(mean_squared_error(dataytrain,xgb.predict(dataxtrain.reshape(len(dataxtrain),-1))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00017181069156477124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8frWvoqPXcF",
        "outputId": "9e6697e2-6917-4fd3-f292-1ab87a339117"
      },
      "source": [
        "xgb = XGBRegressor(objective=\"reg:squarederror\",max_depth =9, learning_rate=0.03  ,n_estimators=450,random_seed=32)\r\n",
        "\r\n",
        "xgb = MultiOutputRegressor(xgb)\r\n",
        "cat = CatBoostRegressor(iterations= 1000,verbose=200,random_seed=23,learning_rate=0.03,depth=6,l2_leaf_reg=0.3) \r\n",
        "cat = MultiOutputRegressor(cat)\r\n",
        "\r\n",
        "dt = DecisionTreeRegressor()\r\n",
        "dt = MultiOutputRegressor(dt)\r\n",
        "\r\n",
        "\r\n",
        "xgb.fit(dataxtrain.reshape(len(dataxtrain),-1),dataytrain)\r\n",
        "cat.fit(dataxtrain.reshape(len(dataxtrain),-1),dataytrain)\r\n",
        "\r\n",
        "dt.fit(dataxtrain.reshape(len(dataxtrain),-1),dataytrain)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
            "  \"because it will generate extra copies and increase memory consumption\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.0009944\ttotal: 29.7ms\tremaining: 29.6s\n",
            "200:\tlearn: 0.0001465\ttotal: 5.1s\tremaining: 20.3s\n",
            "400:\tlearn: 0.0001226\ttotal: 10s\tremaining: 15s\n",
            "600:\tlearn: 0.0001111\ttotal: 15.1s\tremaining: 10s\n",
            "800:\tlearn: 0.0001038\ttotal: 20.1s\tremaining: 5s\n",
            "999:\tlearn: 0.0000985\ttotal: 25.2s\tremaining: 0us\n",
            "0:\tlearn: 0.0009853\ttotal: 26.8ms\tremaining: 26.8s\n",
            "200:\tlearn: 0.0001350\ttotal: 5.07s\tremaining: 20.2s\n",
            "400:\tlearn: 0.0001124\ttotal: 10s\tremaining: 15s\n",
            "600:\tlearn: 0.0001021\ttotal: 15s\tremaining: 9.94s\n",
            "800:\tlearn: 0.0000958\ttotal: 20s\tremaining: 4.97s\n",
            "999:\tlearn: 0.0000911\ttotal: 25s\tremaining: 0us\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiOutputRegressor(estimator=DecisionTreeRegressor(ccp_alpha=0.0,\n",
              "                                                     criterion='mse',\n",
              "                                                     max_depth=None,\n",
              "                                                     max_features=None,\n",
              "                                                     max_leaf_nodes=None,\n",
              "                                                     min_impurity_decrease=0.0,\n",
              "                                                     min_impurity_split=None,\n",
              "                                                     min_samples_leaf=1,\n",
              "                                                     min_samples_split=2,\n",
              "                                                     min_weight_fraction_leaf=0.0,\n",
              "                                                     presort='deprecated',\n",
              "                                                     random_state=None,\n",
              "                                                     splitter='best'),\n",
              "                     n_jobs=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW85P3W39113"
      },
      "source": [
        "0.00010791249808818972"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4D2f6q58mK5",
        "outputId": "c9323bf0-d6c9-434c-e08f-c67ff1c7509a"
      },
      "source": [
        "\r\n",
        "predxgb =  xgb.predict(dataxtrain1.reshape(len(dataxtrain1),-1))\r\n",
        "predcat =  cat.predict(dataxtrain1.reshape(len(dataxtrain1),-1))\r\n",
        "preddt =  dt.predict(dataxtrain1.reshape(len(dataxtrain1),-1))\r\n",
        "\r\n",
        "\r\n",
        "predxgbvalid =  xgb.predict(dataxvalid.reshape(len(dataxvalid),-1))\r\n",
        "predcatvalid =  cat.predict(dataxvalid.reshape(len(dataxvalid),-1))\r\n",
        "\r\n",
        "preddtvalid =  dt.predict(dataxvalid.reshape(len(dataxvalid),-1))\r\n",
        "\r\n",
        "trainpreds = np.concatenate((dataxtrain1.reshape(len(dataxtrain1),-1),predxgb,predcat,preddt),axis=1)\r\n",
        "validpreds = np.concatenate((dataxvalid.reshape(len(dataxvalid),-1),predxgbvalid,predcatvalid,preddtvalid),axis=1)\r\n",
        "\r\n",
        "\r\n",
        "xgbfinal = CatBoostRegressor(iterations= 800,verbose=200,learning_rate=0.03,depth=6,l2_leaf_reg=0.3) \r\n",
        "xgbfinal = MultiOutputRegressor(xgbfinal)\r\n",
        "xgbfinal.fit(trainpreds,dataytrain1)\r\n",
        "\r\n",
        "finalpreds = xgbfinal.predict(validpreds)\r\n",
        "\r\n",
        "np.sqrt(mean_squared_error(datayvalid,finalpreds))\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.0009894\ttotal: 34.5ms\tremaining: 27.5s\n",
            "200:\tlearn: 0.0001168\ttotal: 5.54s\tremaining: 16.5s\n",
            "400:\tlearn: 0.0001056\ttotal: 10.8s\tremaining: 10.8s\n",
            "600:\tlearn: 0.0000988\ttotal: 16.2s\tremaining: 5.35s\n",
            "799:\tlearn: 0.0000939\ttotal: 21.5s\tremaining: 0us\n",
            "0:\tlearn: 0.0009877\ttotal: 27.4ms\tremaining: 21.9s\n",
            "200:\tlearn: 0.0001066\ttotal: 5.42s\tremaining: 16.2s\n",
            "400:\tlearn: 0.0000965\ttotal: 10.8s\tremaining: 10.7s\n",
            "600:\tlearn: 0.0000906\ttotal: 16s\tremaining: 5.31s\n",
            "799:\tlearn: 0.0000865\ttotal: 21.4s\tremaining: 0us\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.766775778358494e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2bG0JVxCRtz"
      },
      "source": [
        "\r\n",
        "# predxgb =  xgb.predict(dataxtrain1.reshape(len(dataxtrain1),-1))\r\n",
        "# predcat =  cat.predict(dataxtrain1.reshape(len(dataxtrain1),-1))\r\n",
        "# # predtab =  tab.predict(dataxtrain1.reshape(len(dataxtrain1),-1))\r\n",
        "# # predrf =  rf.predict(dataxtrain1.reshape(len(dataxtrain1),-1))\r\n",
        "# preddt =  dt.predict(dataxtrain1.reshape(len(dataxtrain1),-1))\r\n",
        "# # predknn =  knn.predict(dataxtrain1.reshape(len(dataxtrain1),-1))\r\n",
        "\r\n",
        "\r\n",
        "# predxgbvalid =  xgb.predict(dataxvalid.reshape(len(dataxvalid),-1))\r\n",
        "# predcatvalid =  cat.predict(dataxvalid.reshape(len(dataxvalid),-1))\r\n",
        "# # predtabvalid =  tab.predict(dataxvalid.reshape(len(dataxvalid),-1))\r\n",
        "# # predrfvalid =  rf.predict(dataxvalid.reshape(len(dataxvalid),-1))\r\n",
        "# preddtvalid =  dt.predict(dataxvalid.reshape(len(dataxvalid),-1))\r\n",
        "# # predknnvalid =  knn.predict(dataxvalid.reshape(len(dataxvalid),-1))\r\n",
        "\r\n",
        "# trainpreds = np.concatenate((dataxtrain1.reshape(len(dataxtrain1),-1),predxgb,predcat,preddt),axis=1)\r\n",
        "# validpreds = np.concatenate((dataxvalid.reshape(len(dataxvalid),-1),predxgbvalid,predcatvalid,preddtvalid),axis=1)\r\n",
        "\r\n",
        "\r\n",
        "xgb1final.fit(trainpreds,dataytrain1)\r\n",
        "xgbfinal.fit(trainpreds,dataytrain1)\r\n",
        "\r\n",
        "finalpreds1 = xgb1final.predict(validpreds)\r\n",
        "finalpreds = xgbfinal.predict(validpreds)\r\n",
        "\r\n",
        "lastTrainx = np.concatenate((finalpreds,finalpreds1),axis=1)\r\n",
        "lr = DecisionTreeRegressor() \r\n",
        "lr = MultiOutputRegressor(lr)\r\n",
        "lr.fit(lastTrainx,datayvalid)\r\n",
        "finalfinalpreds = lr.predict(lastTrainx)\r\n",
        "\r\n",
        "\r\n",
        "np.sqrt(mean_squared_error(datayvalid,finalfinalpreds))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjONMTCTLDL6"
      },
      "source": [
        "\r\n",
        "0.00010733667616236996"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfM6M3nYrg76",
        "outputId": "726b67a5-f9de-43bf-aa24-1bff4c0f5abc"
      },
      "source": [
        "xgb\r\n",
        "xgb1.fit(dataxtrain.reshape(dataxtrain.shape[0],-1),dataytrain)\r\n",
        "preds = xgb1.predict(dataxvalid.reshape(dataxvalid.shape[0],-1))\r\n",
        "np.sqrt(mean_squared_error(datayvalid,preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
            "  \"because it will generate extra copies and increase memory consumption\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00013933667616236996"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT9SmqoHVG-B"
      },
      "source": [
        "0.0003463275090063775"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsgnSAFn1b-1"
      },
      "source": [
        "32 ** 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB8MY9aMQcQC"
      },
      "source": [
        "\r\n",
        "for i in ss[\"seq_id\"].unique():\r\n",
        "  data = load_seq_test(i,timesteps=timesteps)\r\n",
        "  \r\n",
        "  predxgbvalid =  xgb.predict(data.reshape(len(data),-1))\r\n",
        "  predcatvalid =  cat.predict(data.reshape(len(data),-1))\r\n",
        "  preddtvalid =  dt.predict(data.reshape(len(data),-1))\r\n",
        "\r\n",
        "  validpreds = np.concatenate((data.reshape(len(data),-1),predxgbvalid,predcatvalid,preddtvalid),axis=1)\r\n",
        "\r\n",
        "  finalpreds = xgbfinal.predict(validpreds)\r\n",
        "\r\n",
        "\r\n",
        "  # xgbfinal = XGBRegressor(objective=\"reg:squarederror\",max_depth =6, learning_rate=0.03  ,n_estimators=120,random_seed=32)\r\n",
        "  \r\n",
        "  # xgbfinal = MultiOutputRegressor(xgbfinal)\r\n",
        "  # xgbfinal.fit(trainpreds,dataytrain1)\r\n",
        "\r\n",
        "  ss.loc[(ss.seq_id == i)&(ss.cord=='Longitude'), 'Prediction'] = finalpreds[:,1] \r\n",
        "  ss.loc[(ss.seq_id == i)&(ss.cord=='Latitude'), 'Prediction'] = finalpreds[:,0] \r\n",
        "ss.to_csv(\"stacking.csv\",index=False)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}